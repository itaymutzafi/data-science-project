{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Stock Market Prediction Pipeline: A Machine Learning Approach\n",
    "\n",
    "**Authors:** Team 003 (Itay, Moran, Shaked)  \n",
    "**Course:** Workshop in Data Science, Tel Aviv University  \n",
    "**Date:** November 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Abstract\n",
    "\n",
    "This report documents the development of an end-to-end machine learning pipeline for predicting short-term stock movements of Apple Inc. (AAPL). We address the challenge of non-stationarity in financial time series by modeling **Logarithmic Returns** rather than raw prices. Our evaluation framework incorporates statistical metrics (MSE), financial risk metrics (Sharpe Ratio), and trading utility metrics (Directional Accuracy). This document serves as a living research report, integrating theoretical methodology with empirical results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toc",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction & Problem Formulation](#1.-Introduction-&-Problem-Formulation)\n",
    "    - [1.1 Objective](#1.1-Objective)\n",
    "    - [1.2 Target Variable & Stationarity](#1.2-Target-Variable-&-Stationarity)\n",
    "2. [Methodology](#2.-Methodology)\n",
    "    - [2.1 Evaluation Strategy](#2.1-Evaluation-Strategy)\n",
    "    - [2.2 Validation Strategy](#2.2-Validation-Strategy)\n",
    "3. [Empirical Analysis](#3.-Empirical-Analysis)\n",
    "    - [3.1 Stationarity Tests](#3.1-Stationarity-Tests)\n",
    "    - [3.2 Baseline Performance](#3.2-Baseline-Performance)\n",
    "4. [Data Access & Ingestion](#4.-Data-Access-&-Ingestion)\n",
    "5. [Feature Engineering](#5.-Feature-Engineering)\n",
    "6. [Modeling](#6.-Modeling)\n",
    "7. [Advanced Evaluation](#7.-Advanced-Evaluation)\n",
    "8. [Explainability](#8.-Explainability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup & Configuration ---\n",
    "import sys\n",
    "import platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Internal Modules (src/)\n",
    "from src.data.loader import fetch_sample_data\n",
    "from src.evaluation.analysis import check_stationarity, plot_price_vs_returns\n",
    "from src.models.baselines import NaiveBaseline, RandomBaseline\n",
    "from src.evaluation.metrics import evaluate_regression, print_eval\n",
    "from src.evaluation.plots import set_style\n",
    "from src.features.preprocessing import LogReturnTransformer\n",
    "\n",
    "# Apply Academic Plotting Style\n",
    "%matplotlib inline\n",
    "set_style()\n",
    "\n",
    "print(f\"Environment: Python {platform.python_version()}\")\n",
    "print(\"Pipeline modules loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "## 1. Introduction & Problem Formulation\n",
    "\n",
    "### 1.1 Objective\n",
    "The unpredictability of financial markets presents a significant challenge for predictive modeling. Traditional forecasting methods often fail due to the stochastic nature of asset prices. The primary objective of this research is to develop a robust machine learning pipeline capable of predicting the **Logarithmic Returns** of the next trading day ($t+1$) for Apple Inc. (AAPL).\n",
    "\n",
    "### 1.2 Target Variable & Stationarity\n",
    "A fundamental assumption of many statistical learning methods is that the underlying data generating process is stationary (i.e., mean and variance do not change over time). Raw stock prices ($P_t$) violate this assumption, exhibiting trends and heteroscedasticity.\n",
    "\n",
    "To mitigate this, we transform our target variable to **Log-Returns** ($Y_t$):\n",
    "\n",
    "$$Y_t = \\ln(P_{t+1}) - \\ln(P_t) = \\ln\\left(\\frac{P_{t+1}}{P_t}\\right)$$\n",
    "\n",
    "This transformation offers three key advantages:\n",
    "1.  **Stationarity:** Log-returns are approximately stationary, making them suitable for ML algorithms.\n",
    "2.  **Additivity:** Unlike simple percentage returns, log-returns are time-additive.\n",
    "3.  **Normality:** They often approximate a normal distribution, simplifying statistical inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "methodology",
   "metadata": {},
   "source": [
    "## 2. Methodology\n",
    "\n",
    "### 2.1 Evaluation Strategy\n",
    "We employ a multi-faceted evaluation framework to assess model performance from statistical, financial, and trading perspectives.\n",
    "\n",
    "| Metric | Formula | Rationale |\n",
    "| :--- | :--- | :--- |\n",
    "| **MSE** | $\\frac{1}{N} \\sum (y - \\hat{y})^2$ | Penalizes large errors; standard loss function for regression. |\n",
    "| **Sharpe Ratio** | $\\frac{E[R_p] - R_f}{\\sigma_p} \\sqrt{252}$ | Measures risk-adjusted return. Crucial for validating financial viability. |\n",
    "| **Directional Accuracy** | $\\frac{1}{N} \\sum \\mathbb{1}_{sign(y) == sign(\\hat{y})}$ | Assesses the model's ability to predict market direction (Up/Down). |\n",
    "\n",
    "### 2.2 Validation Strategy\n",
    "To prevent **Look-Ahead Bias**, we strictly adhere to a **Walk-Forward Validation** scheme (Expanding Window). Random shuffling (k-fold CV) is strictly prohibited as it destroys the temporal structure of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis",
   "metadata": {},
   "source": [
    "## 3. Empirical Analysis\n",
    "\n",
    "### 3.1 Stationarity Tests\n",
    "We first empirically validate the stationarity of our target variable using the Augmented Dickey-Fuller (ADF) test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Ingestion\n",
    "df_research = fetch_sample_data(\"AAPL\", period=\"2y\")\n",
    "\n",
    "# 2. Feature Engineering (Log Returns)\n",
    "log_transformer = LogReturnTransformer()\n",
    "df_research = log_transformer.transform(df_research)\n",
    "df_research.dropna(inplace=True)\n",
    "\n",
    "# 3. Visualization\n",
    "plot_price_vs_returns(df_research, 'Log_Returns')\n",
    "\n",
    "# 4. Hypothesis Testing (ADF)\n",
    "check_stationarity(df_research['Close'], \"Raw Close Price\")\n",
    "check_stationarity(df_research['Log_Returns'], \"Log Returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baselines",
   "metadata": {},
   "source": [
    "### 3.2 Baseline Performance\n",
    "To establish a performance benchmark, we evaluate two naive models. Any sophisticated model must significantly outperform these baselines to be considered valuable.\n",
    "\n",
    "*   **Naive Baseline (Zero Strategy):** Predicts $y_{t+1} = 0$ (Martingale assumption).\n",
    "*   **Random Baseline:** Predicts $y_{t+1} \\sim \\mathcal{N}(\\mu, \\sigma)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Baselines\n",
    "naive_model = NaiveBaseline(strategy=\"zero\")\n",
    "random_model = RandomBaseline(seed=42)\n",
    "\n",
    "# Fit (Random Baseline learns mean/std)\n",
    "y_true = df_research['Log_Returns']\n",
    "random_model.fit(y_true)\n",
    "\n",
    "# Predict\n",
    "y_pred_naive = naive_model.predict(df_research)\n",
    "y_pred_random = random_model.predict(df_research)\n",
    "\n",
    "# Evaluate\n",
    "metrics_naive = evaluate_regression(y_true, y_pred_naive)\n",
    "metrics_random = evaluate_regression(y_true, y_pred_random)\n",
    "\n",
    "print_eval(metrics_naive, \"Naive Baseline (Zero)\")\n",
    "print_eval(metrics_random, \"Random Baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_access",
   "metadata": {},
   "source": [
    "## 4. Data Access & Ingestion\n",
    "\n",
    "*(To be implemented in Stage 3)*\n",
    "\n",
    "This section will detail the data contracts, caching mechanisms, and ingestion procedures for the full historical dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_engineering",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "\n",
    "*(To be implemented in Stage 4)*\n",
    "\n",
    "We will implement technical indicators (RSI, MACD, Bollinger Bands) and potentially sentiment scores as features for our models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modeling",
   "metadata": {},
   "source": [
    "## 6. Modeling\n",
    "\n",
    "*(To be implemented in Stage 5)*\n",
    "\n",
    "This section will cover the training and validation of advanced machine learning models (e.g., XGBoost, LightGBM) and deep learning architectures (LSTM, Transformer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation_advanced",
   "metadata": {},
   "source": [
    "## 7. Advanced Evaluation\n",
    "\n",
    "*(To be implemented in Stage 6)*\n",
    "\n",
    "Comprehensive backtesting, error analysis, and performance comparison against the baselines established in Section 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explainability",
   "metadata": {},
   "source": [
    "## 8. Explainability\n",
    "\n",
    "*(To be implemented in Stage 7)*\n",
    "\n",
    "Interpretation of model predictions using SHAP values and feature importance analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DS Project .venv)",
   "language": "python",
   "name": "ds-project-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}