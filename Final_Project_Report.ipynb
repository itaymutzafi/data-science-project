{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Final Project Report: Stock Market Prediction Pipeline\n",
    "\n",
    "**Workshop in Data Science â€” Team 003 (Itay, Moran, Shaked)**\n",
    "\n",
    "This notebook serves as the primary research report and documentation for our stock prediction pipeline. It demonstrates the methodology, design decisions, and performance evaluation of our models. \n",
    "\n",
    "**Note:** Heavy computational logic and reusable components are implemented in the `src/` directory to maintain a clean and narrative-driven notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global Imports ---\n",
    "import sys\n",
    "import platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Project Modules ---\n",
    "from src.data_access import fetch_sample_data\n",
    "from src.analysis import check_stationarity, plot_price_vs_returns\n",
    "from src.baselines import NaiveBaseline, RandomBaseline\n",
    "from src.evaluation import evaluate_regression, print_eval\n",
    "from src.plots import set_style\n",
    "\n",
    "# --- Configuration ---\n",
    "%matplotlib inline\n",
    "set_style()  # Apply consistent academic plotting style\n",
    "\n",
    "print(f\"Python Version: {platform.python_version()}\")\n",
    "print(\"Environment initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline_overview",
   "metadata": {},
   "source": [
    "## Pipeline Overview\n",
    "\n",
    "The project follows a structured data science lifecycle:\n",
    "\n",
    "1.  **Business Understanding & Problem Formulation** (Current Stage)\n",
    "2.  **Data Access & Ingestion**\n",
    "3.  **Feature Engineering (Transformers)**\n",
    "4.  **Preprocessing & Windowing**\n",
    "5.  **Modeling (Baselines -> ML -> DL)**\n",
    "6.  **Evaluation & Backtesting**\n",
    "7.  **Explainability**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "business_understanding",
   "metadata": {},
   "source": [
    "## 1. Business Understanding & Problem Formulation\n",
    "\n",
    "### 1.1 Objective\n",
    "The primary objective is to develop a machine learning pipeline capable of predicting short-term movements in the stock market, specifically focusing on **Apple Inc. (AAPL)**. Unlike traditional forecasting which often targets raw prices, our goal is to predict the **Logarithmic Returns** of the next trading day ($t+1$).\n",
    "\n",
    "### 1.2 Target Variable Selection: The Stationarity Challenge\n",
    "Financial time series are notoriously **non-stationary**, meaning their statistical properties (mean, variance) change over time. Standard regression models (e.g., Linear Regression) rely on the assumption of stationarity to generalize effectively.\n",
    "\n",
    "To address this, we transform our target variable from Raw Price ($P_t$) to Log-Returns ($Y_t$):\n",
    "\n",
    "$$Y_t = \\ln(P_{t+1}) - \\ln(P_t) = \\ln\\left(\\frac{P_{t+1}}{P_t}\\right)$$\n",
    "\n",
    "**Why Log-Returns?**\n",
    "1.  **Stationarity:** They exhibit more stable statistical properties than raw prices.\n",
    "2.  **Additivity:** Log-returns are time-additive, simplifying multi-period analysis.\n",
    "3.  **Normality:** They often approximate a normal distribution better than simple percentage returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stationarity_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Empirical Stationarity Analysis ---\n",
    "\n",
    "# 1. Load Sample Data (AAPL)\n",
    "df_research = fetch_sample_data(\"AAPL\", period=\"2y\")\n",
    "\n",
    "# 2. Compute Target (Log Returns)\n",
    "df_research['Log_Returns'] = np.log(df_research['Close'] / df_research['Close'].shift(1))\n",
    "df_research.dropna(inplace=True)\n",
    "\n",
    "# 3. Visual & Statistical Comparison\n",
    "plot_price_vs_returns(df_research, 'Log_Returns')\n",
    "\n",
    "# 4. Statistical Proof (ADF Test)\n",
    "check_stationarity(df_research['Close'], \"Raw Close Price\")\n",
    "check_stationarity(df_research['Log_Returns'], \"Log Returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation_strategy",
   "metadata": {},
   "source": [
    "### 1.3 Evaluation Strategy\n",
    "We employ a dual-metric evaluation strategy to balance statistical rigor with financial reality.\n",
    "\n",
    "**1. Statistical Metric: MSE (Mean Squared Error)**\n",
    "$$MSE = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$$\n",
    "*Rationale:* Penalizes large errors heavily, which is crucial in finance where a single large prediction error can be catastrophic.\n",
    "\n",
    "**2. Financial Metric: Sharpe Ratio**\n",
    "$$Sharpe = \\frac{E[R_p] - R_f}{\\sigma_p} \\times \\sqrt{252}$$\n",
    "*Rationale:* Measures risk-adjusted return. A model with low MSE might still lose money if it misses significant market moves or mispredicts direction. The Sharpe ratio ensures the strategy justifies the risk taken.\n",
    "\n",
    "**3. Trading Metric: Directional Accuracy (DA)**\n",
    "$$DA = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{1}_{sign(y_i) == sign(\\hat{y}_i)}$$\n",
    "*Rationale:* In many trading strategies, correctly predicting the *direction* (Long/Short) is more critical than the exact magnitude.\n",
    "\n",
    "### 1.4 Validation Strategy: Strict Walk-Forward\n",
    "To prevent **Look-Ahead Bias**, we strictly use `TimeSeriesSplit` (expanding window) and avoid random shuffling. Future data must never leak into the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline_performance",
   "metadata": {},
   "source": [
    "## 2. Baseline Performance\n",
    "Before building complex models, we establish simple baselines to serve as performance benchmarks. Any advanced model must significantly outperform these to be considered valuable.\n",
    "\n",
    "1.  **Naive Baseline (Zero Strategy)**: Predicts no change ($y_{t+1} = 0$). This is a strong baseline for log-returns due to the Martingale property of efficient markets.\n",
    "2.  **Random Baseline**: Predicts random values drawn from a normal distribution matching the training data's $\\mu$ and $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline_execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Baseline Evaluation ---\n",
    "\n",
    "# 1. Define Baselines\n",
    "naive_model = NaiveBaseline(strategy=\"zero\")\n",
    "random_model = RandomBaseline(seed=42)\n",
    "\n",
    "# 2. Fit/Predict (Using the research data for demonstration)\n",
    "# Note: In the full pipeline, this will be done via proper Train/Test splits.\n",
    "y_true = df_research['Log_Returns']\n",
    "random_model.fit(y_true)\n",
    "\n",
    "y_pred_naive = naive_model.predict(df_research)\n",
    "y_pred_random = random_model.predict(df_research)\n",
    "\n",
    "# 3. Evaluate & Report\n",
    "metrics_naive = evaluate_regression(y_true, y_pred_naive)\n",
    "metrics_random = evaluate_regression(y_true, y_pred_random)\n",
    "\n",
    "print_eval(metrics_naive, \"Naive Baseline (Zero)\")\n",
    "print_eval(metrics_random, \"Random Baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future_stages",
   "metadata": {},
   "source": [
    "## 3. Future Work (Stages 3-8)\n",
    "\n",
    "The following sections outline the roadmap for the remainder of the project. Implementations will be added iteratively.\n",
    "\n",
    "### 3. Data Access\n",
    "- Implementation of robust data contracts and caching mechanisms.\n",
    "\n",
    "### 4. Transformers\n",
    "- Calculation of Technical Indicators (RSI, MACD, Bollinger Bands).\n",
    "\n",
    "### 5. Preprocessing\n",
    "- Time alignment, windowing (lookback sequences), and normalization.\n",
    "\n",
    "### 6. Modeling\n",
    "- Development of Machine Learning (XGBoost, LightGBM) and Deep Learning (LSTM, Transformer) models.\n",
    "\n",
    "### 7. Evaluation\n",
    "- Comprehensive backtesting and error analysis.\n",
    "\n",
    "### 8. Explainability\n",
    "- SHAP value analysis to understand feature importance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DS Project .venv)",
   "language": "python",
   "name": "ds-project-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}